{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled69.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIQnqJu0IVkztaW+U8bNL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrien50/computervisioncase/blob/main/testtimeaugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57dva-ED2m9r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrwTXET21Zp"
      },
      "source": [
        "### Test-Time Augmentation\r\n",
        "Data augmentation is an approach typically used during the training of the model that expands the training set with modified copies of samples from the training dataset.\r\n",
        "\r\n",
        "Data augmentation is often performed with image data, where copies of images in the training dataset are created with some image manipulation techniques performed, such as zooms, flips, shifts, and more.\r\n",
        "\r\n",
        "The artificially expanded training dataset can result in a more skillful model, as often the performance of deep learning models continues to scale in concert with the size of the training dataset. In addition, the modified or augmented versions of the images in the training dataset assist the model in extracting and learning features in a way that is invariant to their position, lighting, and more.\r\n",
        "\r\n",
        "Test-time augmentation, or TTA for short, is an application of data augmentation to the test dataset.\r\n",
        "\r\n",
        "Specifically, it involves creating multiple augmented copies of each image in the test set, having the model make a prediction for each, then returning an ensemble of those predictions.\r\n",
        "\r\n",
        "Augmentations are chosen to give the model the best opportunity for correctly classifying a given image, and the number of copies of an image for which a model must make a prediction is often small, such as less than 10 or 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raRLlIT_5ja5"
      },
      "source": [
        "We can tie these elements together into a function that will take a configured data generator, fit model, and single image, and will return a class prediction (integer) using test-time augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbo_8Cuz25-k"
      },
      "source": [
        "# make a prediction using test-time augmentation\r\n",
        "def tta_prediction(datagen, model, image, n_examples):\r\n",
        "\t# convert image into dataset\r\n",
        "\tsamples = expand_dims(image, 0)\r\n",
        "\t# prepare iterator\r\n",
        "\tit = datagen.flow(samples, batch_size=n_examples)\r\n",
        "\t# make predictions for each augmented image\r\n",
        "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\r\n",
        "\t# sum across predictions\r\n",
        "\tsummed = numpy.sum(yhats, axis=0)\r\n",
        "\t# argmax across classes\r\n",
        "\treturn argmax(summed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZrCSVLh6VSf"
      },
      "source": [
        "###Dataset and Baseline Model\r\n",
        "We can demonstrate test-time augmentation using a standard computer vision dataset and a convolutional neural network.\r\n",
        "\r\n",
        "Before we can do that, we must select a dataset and a baseline model.\r\n",
        "\r\n",
        "We will use the CIFAR-10 dataset, comprised of 60,000 32×32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. CIFAR-10 is a well-understood dataset and widely used for benchmarking computer vision algorithms in the field of machine learning. The problem is “solved.” Top performance on the problem is achieved by deep learning convolutional neural networks with a classification accuracy above 96% or 97% on the test dataset.\r\n",
        "\r\n",
        "We will also use a convolutional neural network, or CNN, model that is capable of achieving good (better than random) results, but not state-of-the-art results, on the problem. This will be sufficient to demonstrate the lift in performance that test-time augmentation can provide.\r\n",
        "\r\n",
        "The CIFAR-10 dataset can be loaded easily via the Keras API by calling the cifar10.load_data() function, that returns a tuple with the training and test datasets split into input (images) and output (class labels) components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhFWmXna68sl"
      },
      "source": [
        "We are now ready to define a model for this multi-class classification problem.\r\n",
        "\r\n",
        "The model has a convolutional layer with 32 filter maps with a 3×3 kernel using the rectifier linear activation, “same” padding so the output is the same size as the input and the He weight initialization. This is followed by a batch normalization layer and a max pooling layer.\r\n",
        "\r\n",
        "This pattern is repeated with a convolutional, batch norm, and max pooling layer, although the number of filters is increased to 64. The output is then flattened before being interpreted by a dense layer and finally provided to the output layer to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5miMO8AI6Xi_"
      },
      "source": [
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPooling2D((2, 2)))\r\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPooling2D((2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOs4jhT27PLN",
        "outputId": "2fd2190e-8603-47b5-d13f-a64c01315365"
      },
      "source": [
        "# baseline cnn model for the cifar10 problem\r\n",
        "from keras.datasets.cifar10 import load_data\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "# load dataset\r\n",
        "(trainX, trainY), (testX, testY) = load_data()\r\n",
        "# normalize pixel values\r\n",
        "trainX = trainX.astype('float32') / 255\r\n",
        "testX = testX.astype('float32') / 255\r\n",
        "# one hot encode target values\r\n",
        "trainY = to_categorical(trainY)\r\n",
        "testY = to_categorical(testY)\r\n",
        "# define model\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPooling2D((2, 2)))\r\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPooling2D((2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "# compile model\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "# fit model\r\n",
        "history = model.fit(trainX, trainY, epochs=3, batch_size=128)\r\n",
        "# evaluate model\r\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\r\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 105s 265ms/step - loss: 1.4538 - accuracy: 0.4987\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 103s 263ms/step - loss: 0.8468 - accuracy: 0.7065\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 103s 264ms/step - loss: 0.6562 - accuracy: 0.7747\n",
            "0.6843000054359436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV1jrO4j8AdI"
      },
      "source": [
        "Neural networks are stochastic algorithms and the same model fit on the same data multiple times may find a different set of weights and, in turn, have different performance each time.\r\n",
        "\r\n",
        "In order to even out the estimate of model performance, we can change the example to re-run the fit and evaluation of the model multiple times and report the mean and standard deviation of the distribution of scores on the test dataset.\r\n",
        "\r\n",
        "First, we can define a function named load_dataset() that will load the CIFAR-10 dataset and prepare it for modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqjh56RU8Cdj",
        "outputId": "3d500c8e-fa52-4c01-d92c-ad55f65edf00"
      },
      "source": [
        "# baseline cnn model for the cifar10 problem, repeated evaluation\r\n",
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from keras.datasets.cifar10 import load_data\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "\r\n",
        "# load and return the cifar10 dataset ready for modeling\r\n",
        "def load_dataset():\r\n",
        "\t# load dataset\r\n",
        "\t(trainX, trainY), (testX, testY) = load_data()\r\n",
        "\t# normalize pixel values\r\n",
        "\ttrainX = trainX.astype('float32') / 255\r\n",
        "\ttestX = testX.astype('float32') / 255\r\n",
        "\t# one hot encode target values\r\n",
        "\ttrainY = to_categorical(trainY)\r\n",
        "\ttestY = to_categorical(testY)\r\n",
        "\treturn trainX, trainY, testX, testY\r\n",
        "\r\n",
        "# define the cnn model for the cifar10 dataset\r\n",
        "def define_model():\r\n",
        "\t# define model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\r\n",
        "\tmodel.add(BatchNormalization())\r\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\r\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\r\n",
        "\tmodel.add(BatchNormalization())\r\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\r\n",
        "\tmodel.add(Flatten())\r\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "\tmodel.add(BatchNormalization())\r\n",
        "\tmodel.add(Dense(10, activation='softmax'))\r\n",
        "\t# compile model\r\n",
        "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "# fit and evaluate a defined model\r\n",
        "def evaluate_model(model, trainX, trainY, testX, testY):\r\n",
        "\t# fit model\r\n",
        "\tmodel.fit(trainX, trainY, epochs=3, batch_size=128, verbose=0)\r\n",
        "\t# evaluate model\r\n",
        "\t_, acc = model.evaluate(testX, testY, verbose=0)\r\n",
        "\treturn acc\r\n",
        "\r\n",
        "# repeatedly evaluate model, return distribution of scores\r\n",
        "def repeated_evaluation(trainX, trainY, testX, testY, repeats=10):\r\n",
        "\tscores = list()\r\n",
        "\tfor _ in range(repeats):\r\n",
        "\t\t# define model\r\n",
        "\t\tmodel = define_model()\r\n",
        "\t\t# fit and evaluate model\r\n",
        "\t\taccuracy = evaluate_model(model, trainX, trainY, testX, testY)\r\n",
        "\t\t# store score\r\n",
        "\t\tscores.append(accuracy)\r\n",
        "\t\tprint('> %.3f' % accuracy)\r\n",
        "\treturn scores\r\n",
        "\r\n",
        "# load dataset\r\n",
        "trainX, trainY, testX, testY = load_dataset()\r\n",
        "# evaluate model\r\n",
        "scores = repeated_evaluation(trainX, trainY, testX, testY)\r\n",
        "# summarize result\r\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> 0.699\n",
            "> 0.701\n",
            "> 0.690\n",
            "> 0.684\n",
            "> 0.685\n",
            "> 0.694\n",
            "> 0.689\n",
            "> 0.659\n",
            "> 0.672\n",
            "> 0.669\n",
            "Accuracy: 0.684 (0.013)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiPlNTO4--PT"
      },
      "source": [
        "ying all of this together, the complete example of the repeated evaluation of a \r\n",
        "\r\n",
        "CNN for CIFAR-10 with test-time augmentation is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "184nH5cV_TzW"
      },
      "source": [
        "# cnn model for the cifar10 problem with test-time augmentation\r\n",
        "import numpy\r\n",
        "from numpy import argmax\r\n",
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from numpy import expand_dims\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from keras.datasets.cifar10 import load_data\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import BatchNormalization\r\n",
        "\r\n",
        "# load and return the cifar10 dataset ready for modeling\r\n",
        "def load_dataset():\r\n",
        "\t# load dataset\r\n",
        "\t(trainX, trainY), (testX, testY) = load_data()\r\n",
        "\t# normalize pixel values\r\n",
        "\ttrainX = trainX.astype('float32') / 255\r\n",
        "\ttestX = testX.astype('float32') / 255\r\n",
        "\t# one hot encode target values\r\n",
        "\ttrainY = to_categorical(trainY)\r\n",
        "\ttestY = to_categorical(testY)\r\n",
        "\treturn trainX, trainY, testX, testY\r\n",
        "\r\n",
        "# define the cnn model for the cifar10 dataset\r\n",
        "def define_model():\r\n",
        "\t# define model\r\n",
        "\tmodel = Sequential()\r\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\r\n",
        "\tmodel.add(BatchNormalization())\r\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\r\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\r\n",
        "\tmodel.add(BatchNormalization())\r\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\r\n",
        "\tmodel.add(Flatten())\r\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "\tmodel.add(BatchNormalization())\r\n",
        "\tmodel.add(Dense(10, activation='softmax'))\r\n",
        "\t# compile model\r\n",
        "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\treturn model\r\n",
        "\r\n",
        "# make a prediction using test-time augmentation\r\n",
        "def tta_prediction(datagen, model, image, n_examples):\r\n",
        "\t# convert image into dataset\r\n",
        "\tsamples = expand_dims(image, 0)\r\n",
        "\t# prepare iterator\r\n",
        "\tit = datagen.flow(samples, batch_size=n_examples)\r\n",
        "\t# make predictions for each augmented image\r\n",
        "\tyhats = model.predict_generator(it, steps=n_examples, verbose=0)\r\n",
        "\t# sum across predictions\r\n",
        "\tsummed = numpy.sum(yhats, axis=0)\r\n",
        "\t# argmax across classes\r\n",
        "\treturn argmax(summed)\r\n",
        "\r\n",
        "# evaluate a model on a dataset using test-time augmentation\r\n",
        "def tta_evaluate_model(model, testX, testY):\r\n",
        "\t# configure image data augmentation\r\n",
        "\tdatagen = ImageDataGenerator(horizontal_flip=True)\r\n",
        "\t# define the number of augmented images to generate per test set image\r\n",
        "\tn_examples_per_image = 7\r\n",
        "\tyhats = list()\r\n",
        "\tfor i in range(len(testX)):\r\n",
        "\t\t# make augmented prediction\r\n",
        "\t\tyhat = tta_prediction(datagen, model, testX[i], n_examples_per_image)\r\n",
        "\t\t# store for evaluation\r\n",
        "\t\tyhats.append(yhat)\r\n",
        "\t# calculate accuracy\r\n",
        "\ttestY_labels = argmax(testY, axis=1)\r\n",
        "\tacc = accuracy_score(testY_labels, yhats)\r\n",
        "\treturn acc\r\n",
        "\r\n",
        "# fit and evaluate a defined model\r\n",
        "def evaluate_model(model, trainX, trainY, testX, testY):\r\n",
        "\t# fit model\r\n",
        "\tmodel.fit(trainX, trainY, epochs=3, batch_size=128, verbose=0)\r\n",
        "\t# evaluate model using tta\r\n",
        "\tacc = tta_evaluate_model(model, testX, testY)\r\n",
        "\treturn acc\r\n",
        "\r\n",
        "# repeatedly evaluate model, return distribution of scores\r\n",
        "def repeated_evaluation(trainX, trainY, testX, testY, repeats=10):\r\n",
        "\tscores = list()\r\n",
        "\tfor _ in range(repeats):\r\n",
        "\t\t# define model\r\n",
        "\t\tmodel = define_model()\r\n",
        "\t\t# fit and evaluate model\r\n",
        "\t\taccuracy = evaluate_model(model, trainX, trainY, testX, testY)\r\n",
        "\t\t# store score\r\n",
        "\t\tscores.append(accuracy)\r\n",
        "\t\tprint('> %.3f' % accuracy)\r\n",
        "\treturn scores\r\n",
        "\r\n",
        "# load dataset\r\n",
        "trainX, trainY, testX, testY = load_dataset()\r\n",
        "# evaluate model\r\n",
        "scores = repeated_evaluation(trainX, trainY, testX, testY)\r\n",
        "# summarize result\r\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psn53OSO_ab3"
      },
      "source": [
        "### How to Tune Test-Time Augmentation Configuration\r\n",
        "\r\n",
        "Choosing the augmentation configurations that give the biggest lift in model performance can be challenging.\r\n",
        "\r\n",
        "Not only are there many augmentation methods to choose from and configuration options for each, but the time to fit and evaluate a model on a single set of configuration options can take a long time, even if fit on a fast GPU.\r\n",
        "\r\n",
        "Instead, I recommend fitting the model once and saving it to file.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1PFnqC0-_F_"
      },
      "source": [
        "# save model\r\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JXVmBJ5Ac1X"
      },
      "source": [
        "Then load the model from a separate file and evaluate different test-time augmentation schemes on a small validation dataset or small subset of the test set.\r\n",
        "\r\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoUPbBwzBQg4"
      },
      "source": [
        "...\r\n",
        "# load model\r\n",
        "model = load_model('model.h5')\r\n",
        "# evaluate model\r\n",
        "datagen = ImageDataGenerator(...)\r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WniE1Cy6CIUO"
      },
      "source": [
        "Once you find a set of augmentation options that give the biggest lift, you can then evaluate the model on the whole test set or trial a repeated evaluation experiment as above.\r\n",
        "\r\n",
        "Test-time augmentation configuration not only includes the options for the ImageDataGenerator, but also the number of images generated from which the average prediction will be made for each example in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ-Lc_C5CJC6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}