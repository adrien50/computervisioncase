{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preparationcoputervision.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYo1jm/J2idOBPnzQqwLrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrien50/computervisioncase/blob/main/preparationcoputervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBz-qsz3ZiWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNfHT42ea0Df"
      },
      "source": [
        "###Top ILSVRC Models\r\n",
        "When applying convolutional neural networks for image classification, it can be challenging to know exactly how to prepare images for modeling, e.g. scaling or normalizing pixel values.\r\n",
        "\r\n",
        "Further, image data augmentation can be used to improve model performance and reduce generalization error and test-time augmentation can be used to improve the predictive performance of a fit model.\r\n",
        "\r\n",
        "Rather than guessing at what might be effective, a good practice is to take a closer look at the types of data preparation, train-time augmentation, and test-time augmentation used on top-performing models described in the literature.\r\n",
        "\r\n",
        "The ImageNet Large Scale Visual Recognition Challenge, or ILSVRC for short, is an annual competition helped between 2010 and 2017 in which challenge tasks use subsets of the ImageNet dataset. This competition has resulted in a range of state-of-the-art deep learning convolutional neural network models for image classification, the architectures and configurations of which have become heuristics and best practices in the field.\r\n",
        "\r\n",
        "The papers describing the models that won or performed well on tasks in this annual competition can be reviewed in order to discover the types of data preparation an image augmentation performed. In turn, these can be used as suggestions and best practices when preparing image data for your own image classification tasks.\r\n",
        "\r\n",
        "In the following sections, we will review the data preparation and image augmentation used in four top models: they are SuperVision/AlexNet, GoogLeNet/Inception, VGG, and ResNet.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0TjT2dla_3v"
      },
      "source": [
        "###SuperVision (AlexNet) Data Preparation\r\n",
        "Alex Krizhevsky, et al. from the University of Toronto in their paper 2012 titled “ImageNet Classification with Deep Convolutional Neural Networks” developed a convolutional neural network that achieved top results on the ILSVRC-2010 and ILSVRC-2012 image classification tasks.\r\n",
        "\r\n",
        "These results sparked interested in deep learning in computer vision. They called their model SuperVision, but it has since been referred to as AlexNet.\r\n",
        "\r\n",
        "###Data Preparation\r\n",
        "Images in the training dataset had differing sizes, therefore images had to be resized before being used as input to the model.\r\n",
        "\r\n",
        "Square images were resized to the shape 256×256 pixels. Rectangular images were resized to 256 pixels on their shortest side, then the middle 256×256 square was cropped from the image. Note: the network expects input images to have the shape 224×224, achieved via training augmentation.\r\n",
        "\r\n",
        "A mean pixel value was then subtracted from each pixel, referred to as centering. It is believed that this was performed per-channel: that is mean pixel values were estimated from the training dataset, one for each of the red, green, and blue channels of the color images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqLwmy69cd6w"
      },
      "source": [
        "###Train-Time Augmentation\r\n",
        "Image augmentation was performed to the training dataset.\r\n",
        "\r\n",
        "Specifically, augmentations were performed in memory and the results were not saved, the so-called just-in-time augmentation that is now the standard way for using the approach.\r\n",
        "\r\n",
        "The first type of augmentation performed was horizontal flips of a smaller cropped square image that was expanded to the required side using horizontal reflections within the image.\r\n",
        "The second type of augmentation performed was random changes to the light-level or brightness of the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COOt3SNPcuy3"
      },
      "source": [
        "###Test-Time Augmentation\r\n",
        "Test-time augmentation was performed in order to give a fit model every chance of making a robust prediction.\r\n",
        "\r\n",
        "This involved creating five cropped versions of the input image and five cropped versions of the horizontally flipped version of the image, then averaging the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOTUblvKa2w1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENFa6GdhdOnY"
      },
      "source": [
        "####GoogLeNet (Inception) Data Preparation\r\n",
        "Christian Szegedy, et al. from Google achieved top results for object detection with their GoogLeNet model that made use of the inception model and inception architecture. This approach was described in their 2014 paper titled “Going Deeper with Convolutions.”\r\n",
        "\r\n",
        "###Data Preparation\r\n",
        "Data preparation is described as subtracting the mean pixel value, likely centered per-channel as with AlexNet.\r\n",
        "\r\n",
        "###Train-Time Augmentation\r\n",
        "Train-time image augmentation is performed using a range of techniques.\r\n",
        "\r\n",
        "Randomly sized crops of images in the training dataset are taken using a randomly selected aspect ratio of either 3/4 or 4/3.\r\n",
        "\r\n",
        "Additionally, “photometric distortions” are used, involving random changes to image properties such as color, contrast, and brightness.\r\n",
        "\r\n",
        "Images are adjusted to fit the expected input shape of the model and different interpolation methods are selected at random.\r\n",
        "\r\n",
        "###Test-Time Augmentation\r\n",
        "Similar to AlexNet, test-time augmentation is performed, albeit more extensively.\r\n",
        "\r\n",
        "Each image is resampled at four different scales, from which multiple square crops are taken and resized to the expected input shape of the image. The result is a prediction on up to 144 versions of a given input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPAz4M4Be4DD"
      },
      "source": [
        "##VGG Data Preparation\r\n",
        "Karen Simonyan and Andrew Zisserman from the Oxford Vision Geometry Group (VGG) achieved top results for image classification and localization with their VGG model. Their approach is described in their 2015 paper titled “Very Deep Convolutional Networks for Large-Scale Image Recognition.”\r\n",
        "\r\n",
        "###Data Preparation\r\n",
        "As described with the prior models, the data preparation involved standardizing the shape of the input images to small squares and subtracting the per-channel pixel mean calculated on the training dataset.\r\n",
        "\r\n",
        "###train-Time Augmentation\r\n",
        "A range of different image scaling was explored with the model.\r\n",
        "\r\n",
        "One approach described involved first training a model with a fixed but smaller image size, retaining the model weights, then using them as a starting point for training a new model with a larger but still fixed-sized image. This approach was designed in an effort to speed up the training of the larger (second) model.\r\n",
        "Another approach to image scaling was described called “multi-scale training” that involved randomly selecting an image scale size for each image.\r\n",
        "In both approaches to training, the input image was then taken as a smaller crop of the input. Additionally, horizontal flips and color shifts were applied to the crops.\r\n",
        "\r\n",
        "###Test-Time Augmentation\r\n",
        "The “multi-scale” approach evaluated during training-time was also evaluated at test-time and was referred to more generally as “scale jitter.”\r\n",
        "\r\n",
        "Multiple different scaled versions of a given test image were created, predictions made for each, then the predictions were averaged to give a final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMu4TRbkifWO"
      },
      "source": [
        "##Data Preparation Recommendations\r\n",
        "Given the review of data preparation performed across top-performing models, we can summarise a number of best practices to consider when preparing data for your own image classification tasks. This section summarizes these findings.\r\n",
        "\r\n",
        "-Data Preparation. A fixed size must be selected for input images, and all images must be resized to that shape. The most common type of pixel scaling involves centering pixel values per-channel, perhaps followed by some type of normalization.\r\n",
        "\r\n",
        "-Train-Time Augmentation.  Train-time augmentation is required, most commonly involved resizing and cropping of input images, as well as modification of images such as shifts, flips and changes to colors.\r\n",
        "\r\n",
        "-Test-Time Augmentation. Test-time augmentation was focused on systematic crops of the input images to ensure features present in the input images were detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQRgYShodQL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}